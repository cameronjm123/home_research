{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76931391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "import seaborn as sns\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de117c94",
   "metadata": {},
   "source": [
    "#### Step 1: Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276145af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eursek_tick_data = pd.read_csv(\"EURSEK_tick_UTC+0_00_2019-Parse.csv.zst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cce8f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTC</th>\n",
       "      <th>AskPrice</th>\n",
       "      <th>BidPrice</th>\n",
       "      <th>AskVolume</th>\n",
       "      <th>BidVolume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T22:03:06.044+00:00</td>\n",
       "      <td>10.15908</td>\n",
       "      <td>10.17841</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T22:03:14.888+00:00</td>\n",
       "      <td>10.15903</td>\n",
       "      <td>10.17834</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T22:03:34.932+00:00</td>\n",
       "      <td>10.15904</td>\n",
       "      <td>10.17835</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T22:03:54.888+00:00</td>\n",
       "      <td>10.15905</td>\n",
       "      <td>10.17836</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T22:04:02.688+00:00</td>\n",
       "      <td>10.15826</td>\n",
       "      <td>10.17825</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626115</th>\n",
       "      <td>2019-12-31T21:58:49.588+00:00</td>\n",
       "      <td>10.48746</td>\n",
       "      <td>10.51634</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626116</th>\n",
       "      <td>2019-12-31T21:58:52.767+00:00</td>\n",
       "      <td>10.48746</td>\n",
       "      <td>10.51594</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626117</th>\n",
       "      <td>2019-12-31T21:58:58.312+00:00</td>\n",
       "      <td>10.48746</td>\n",
       "      <td>10.51555</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626118</th>\n",
       "      <td>2019-12-31T21:59:00.557+00:00</td>\n",
       "      <td>10.48746</td>\n",
       "      <td>10.51514</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626119</th>\n",
       "      <td>2019-12-31T21:59:00.657+00:00</td>\n",
       "      <td>10.48746</td>\n",
       "      <td>10.51593</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23626120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    UTC  AskPrice  BidPrice  AskVolume  \\\n",
       "0         2019-01-01T22:03:06.044+00:00  10.15908  10.17841       0.75   \n",
       "1         2019-01-01T22:03:14.888+00:00  10.15903  10.17834       0.75   \n",
       "2         2019-01-01T22:03:34.932+00:00  10.15904  10.17835       0.75   \n",
       "3         2019-01-01T22:03:54.888+00:00  10.15905  10.17836       0.75   \n",
       "4         2019-01-01T22:04:02.688+00:00  10.15826  10.17825       0.75   \n",
       "...                                 ...       ...       ...        ...   \n",
       "23626115  2019-12-31T21:58:49.588+00:00  10.48746  10.51634       1.30   \n",
       "23626116  2019-12-31T21:58:52.767+00:00  10.48746  10.51594       1.30   \n",
       "23626117  2019-12-31T21:58:58.312+00:00  10.48746  10.51555       1.30   \n",
       "23626118  2019-12-31T21:59:00.557+00:00  10.48746  10.51514       1.30   \n",
       "23626119  2019-12-31T21:59:00.657+00:00  10.48746  10.51593       1.30   \n",
       "\n",
       "          BidVolume  \n",
       "0              0.75  \n",
       "1              0.75  \n",
       "2              0.75  \n",
       "3              0.75  \n",
       "4              0.75  \n",
       "...             ...  \n",
       "23626115       1.00  \n",
       "23626116       1.00  \n",
       "23626117       1.00  \n",
       "23626118       1.00  \n",
       "23626119       1.00  \n",
       "\n",
       "[23626120 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eursek_tick_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7cbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eursek_tick_data['AskVolume'] = eursek_tick_data['AskVolume'].groupby(eursek_tick_data.index // 10).cumsum()\n",
    "eursek_tick_data['BidVolume'] = eursek_tick_data['BidVolume'].groupby(eursek_tick_data.index // 10).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8a7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eursek_tick_data)\n",
    "data = eursek_tick_data.loc[::10]\n",
    "data.index = pd.to_datetime(data['UTC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d137308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\AppData\\Local\\Temp\\ipykernel_33332\\3205618487.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['MidPrice'] = (data['AskPrice'] + data['BidPrice'])/2\n"
     ]
    }
   ],
   "source": [
    "data.drop('UTC',axis=1)\n",
    "data['MidPrice'] = (data['AskPrice'] + data['BidPrice'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8452936",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bars = pd.DataFrame(index=data.index)\n",
    "final_bars['log close'] = np.log(data['MidPrice'])\n",
    "final_bars['log std'] = final_bars['log close'].rolling(50).std()\n",
    "final_bars['close'] = data['MidPrice']\n",
    "final_bars = final_bars.dropna()\n",
    "final_bars.index = final_bars.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a8f899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log close</th>\n",
       "      <th>log std</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:17.013</th>\n",
       "      <td>2.319520</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>10.170790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:26.037</th>\n",
       "      <td>2.319422</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>10.169790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:27.004</th>\n",
       "      <td>2.319451</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>10.170085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:29.486</th>\n",
       "      <td>2.319476</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>10.170340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:36.650</th>\n",
       "      <td>2.319458</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>10.170160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:53:24.510</th>\n",
       "      <td>2.351642</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>10.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:54:26.787</th>\n",
       "      <td>2.351404</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:34.797</th>\n",
       "      <td>2.351594</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:56:22.928</th>\n",
       "      <td>2.351575</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.502095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:58:27.413</th>\n",
       "      <td>2.351637</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.502750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2362563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         log close   log std      close\n",
       "UTC                                                    \n",
       "2019-01-01 23:05:17.013   2.319520  0.000154  10.170790\n",
       "2019-01-01 23:05:26.037   2.319422  0.000155  10.169790\n",
       "2019-01-01 23:05:27.004   2.319451  0.000154  10.170085\n",
       "2019-01-01 23:05:29.486   2.319476  0.000152  10.170340\n",
       "2019-01-01 23:05:36.650   2.319458  0.000148  10.170160\n",
       "...                            ...       ...        ...\n",
       "2019-12-31 21:53:24.510   2.351642  0.000275  10.502800\n",
       "2019-12-31 21:54:26.787   2.351404  0.000274  10.500300\n",
       "2019-12-31 21:55:34.797   2.351594  0.000274  10.502300\n",
       "2019-12-31 21:56:22.928   2.351575  0.000274  10.502095\n",
       "2019-12-31 21:58:27.413   2.351637  0.000274  10.502750\n",
       "\n",
       "[2362563 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066e541",
   "metadata": {},
   "source": [
    "#### Step 2: Filter data, form vertical barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c3b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_cusum(raw_data, h):\n",
    "    filtered_events_indices = []\n",
    "    s_pos = 0\n",
    "    s_neg = 0\n",
    "    diff = raw_data.diff()\n",
    "    count = 0\n",
    "    for i in diff.index[1:]:\n",
    "        count+=1\n",
    "        s_pos = max(0, s_pos + diff.loc[i])\n",
    "        s_neg = min(0, s_neg + diff.loc[i])\n",
    "        if s_neg < -h:\n",
    "            s_neg = 0\n",
    "            filtered_events_indices.append(i)\n",
    "        elif s_pos > h:\n",
    "            s_pos = 0\n",
    "            filtered_events_indices.append(i)\n",
    "        if count%100000 == 0:\n",
    "            print(count)\n",
    "    return pd.DatetimeIndex(filtered_events_indices)\n",
    "\n",
    "def get_vertical_barrier(close, t_events, num_days):\n",
    "    t1 = close.index.searchsorted(t_events+pd.Timedelta(days=num_days))\n",
    "    t1 = t1[t1<close.shape[0]]\n",
    "    return pd.Series(close.index[t1], index=t_events[:t1.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e1c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n"
     ]
    }
   ],
   "source": [
    "# get relevant events\n",
    "event_filter_diff_min_h = final_bars['log std'].mean()/2\n",
    "t_events = sym_cusum(final_bars['log close'], event_filter_diff_min_h)\n",
    "\n",
    "t_events\n",
    "# get vertical barriers\n",
    "t1 = get_vertical_barrier(final_bars['log close'], t_events, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce998d",
   "metadata": {},
   "source": [
    "#### Step 3: RSI/MACD for side labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdd6211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\AppData\\Local\\Temp\\ipykernel_33332\\1313541155.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sides['side_majority'] = sides.mode(axis=1)\n",
      "C:\\Users\\camer\\AppData\\Local\\Temp\\ipykernel_33332\\1313541155.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sides['side_confidence'] = sides.mean(axis=1).abs()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "side_confidence\n",
       "0.333333    1477479\n",
       "0.666667     849015\n",
       "1.000000      35320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# side of trade will be decided from a mixture of indicators - MACD, RSI, Crossing SMAs, Bollinger bands, Stochastic oscillator\n",
    "\n",
    "# MACD\n",
    "fast_window_macd = 12\n",
    "slow_window_macd = 26\n",
    "macd_signal_window = 9\n",
    "\n",
    "raw_data = pd.DataFrame({'log close': final_bars['log close']})\n",
    "\n",
    "raw_data['fast_ema'] = raw_data['log close'].ewm(span=fast_window_macd).mean()\n",
    "raw_data['slow_ema'] = raw_data['log close'].ewm(span=slow_window_macd).mean()\n",
    "raw_data['macd'] = raw_data['fast_ema'] - raw_data['slow_ema']\n",
    "raw_data['macd_ema'] = raw_data['macd'].ewm(span=macd_signal_window).mean()\n",
    "\n",
    "long_signals_macd = raw_data['macd'] >= raw_data['macd_ema'] \n",
    "short_signals_macd = raw_data['macd'] < raw_data['macd_ema'] \n",
    "raw_data.loc[long_signals_macd, 'side_macd'] = 1\n",
    "raw_data.loc[short_signals_macd, 'side_macd'] = -1\n",
    "\n",
    "# RSI\n",
    "rsi_window = 14\n",
    "\n",
    "raw_data['diff'] = raw_data['log close'].diff()\n",
    "raw_data['gain'] = raw_data['diff'].apply(lambda x: x if x >= 0 else 0)\n",
    "raw_data['loss'] = raw_data['diff'].apply(lambda x: -x if x < 0 else 0)\n",
    "raw_data['avg_gain'] = raw_data['gain'].rolling(rsi_window).mean()\n",
    "raw_data['avg_loss'] = raw_data['loss'].rolling(rsi_window).mean()\n",
    "raw_data['rsi'] = 100 - 100/(1+raw_data['avg_gain']/raw_data['avg_loss'])\n",
    "\n",
    "long_signals_rsi = raw_data['rsi'] <= 30\n",
    "short_signals_rsi = raw_data['rsi'] >= 70\n",
    "raw_data.loc[long_signals_macd, 'side_rsi'] = 1\n",
    "raw_data.loc[short_signals_macd, 'side_rsi'] = -1\n",
    "raw_data['side_rsi'] = raw_data['side_rsi'].fillna(method='ffill')\n",
    "\n",
    "# SMAs\n",
    "fast_window_sma = 20\n",
    "slow_window_sma = 50\n",
    "raw_data['fast_mavg_sma'] = raw_data['log close'].rolling(window=fast_window_sma, min_periods=fast_window_sma, center=False).mean()\n",
    "raw_data['slow_mavg_sma'] = raw_data['log close'].rolling(window=slow_window_sma, min_periods=slow_window_sma, center=False).mean()\n",
    "\n",
    "long_signals_sma = raw_data['fast_mavg_sma'] >= raw_data['slow_mavg_sma'] \n",
    "short_signals_sma = raw_data['fast_mavg_sma'] < raw_data['slow_mavg_sma'] \n",
    "raw_data.loc[long_signals_sma, 'side_sma'] = 1\n",
    "raw_data.loc[short_signals_sma, 'side_sma'] = -1\n",
    "\n",
    "# Bollinger bands\n",
    "bb_window = 20\n",
    "std_no = 2\n",
    "\n",
    "raw_data['mavg_bb'] = raw_data['log close'].rolling(window=bb_window, min_periods=bb_window, center=False).mean()\n",
    "raw_data['mstd_bb'] = raw_data['log close'].rolling(window=bb_window, min_periods=bb_window, center=False).std()\n",
    "raw_data['BOLU'] = raw_data['mavg_bb'] + std_no * raw_data['mstd_bb']\n",
    "raw_data['BOLD'] = raw_data['mavg_bb'] - std_no * raw_data['mstd_bb']\n",
    "\n",
    "long_signals_bb = raw_data['log close'] <= raw_data['BOLD'] \n",
    "short_signals_bb = raw_data['log close'] >= raw_data['BOLU'] \n",
    "raw_data.loc[long_signals_bb, 'side_bb'] = 1\n",
    "raw_data.loc[short_signals_bb, 'side_bb'] = -1\n",
    "raw_data['side_bb'] = raw_data['side_bb'].fillna(method='ffill')\n",
    "\n",
    "# Stochastic oscillator\n",
    "so_window = 14\n",
    "so_smoothing_window = 3\n",
    "\n",
    "raw_data['fast_so'] = 100*(raw_data['log close'] - raw_data['log close'].rolling(so_window).min())/(raw_data['log close'].rolling(so_window).max() - raw_data['log close'].rolling(so_window).min())\n",
    "raw_data['slow_so'] = raw_data['fast_so'].rolling(so_smoothing_window).mean()\n",
    "\n",
    "long_signals_so = (raw_data['fast_so'] >= raw_data['slow_so']) & (raw_data['fast_so'] <= 20)\n",
    "short_signals_so = (raw_data['fast_so'] < raw_data['slow_so']) & (raw_data['fast_so'] >= 80)\n",
    "raw_data.loc[long_signals_so, 'side_so'] = 1\n",
    "raw_data.loc[short_signals_so, 'side_so'] = -1\n",
    "raw_data['side_so'] = raw_data['side_so'].fillna(method='ffill')\n",
    "\n",
    "raw_data = raw_data.dropna()\n",
    "sides = raw_data[['side_macd','side_rsi','side_sma','side_bb','side_so']]\n",
    "sides['side_majority'] = sides.mode(axis=1)\n",
    "sides['side_confidence'] = sides.mean(axis=1).abs()\n",
    "sides['side_confidence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e9c029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UTC\n",
       "2019-01-01 23:11:39.163    1.0\n",
       "2019-01-01 23:11:40.277    1.0\n",
       "2019-01-01 23:11:42.445    1.0\n",
       "2019-01-01 23:11:43.221   -1.0\n",
       "2019-01-01 23:11:44.388   -1.0\n",
       "                          ... \n",
       "2019-12-31 21:53:24.510    1.0\n",
       "2019-12-31 21:54:26.787    1.0\n",
       "2019-12-31 21:55:34.797    1.0\n",
       "2019-12-31 21:56:22.928    1.0\n",
       "2019-12-31 21:58:27.413    1.0\n",
       "Name: side_majority, Length: 2361814, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose what side we are going to use\n",
    "side = sides['side_majority']\n",
    "side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984711e",
   "metadata": {},
   "source": [
    "#### Step 4: Triple barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8513e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ptsl_on_t1(close, events, ptSl, molecule): # (ptsl means profit taking and stop loss)\n",
    "    '''\n",
    "    close: close price data - horizontal barriers will be calculated directly from close return vol\n",
    "    events: vertical barrier TIMES \n",
    "    ptSl: list of 1 or 0 indicating if upper or lower barriers should be activated\n",
    "    '''\n",
    "    events = events.loc[molecule] # to do with multiprocessing\n",
    "    out = events[['t1']].copy(deep=True)\n",
    "    if ptSl[0] > 0:\n",
    "        pt = ptSl[0] * events['trgt']\n",
    "    else:\n",
    "        pt = np.infty * events['trgt']\n",
    "    if ptSl[1] > 0:\n",
    "        sl = -ptSl[1] * events['trgt']\n",
    "    else:\n",
    "        sl = - np.infty * events['trgt']\n",
    "    for loc, t1 in events['t1'].fillna(close.index[-1]).items():\n",
    "        df0 = close[loc:t1] # takes all the close prices between the time in events['t1'] (the index) and the corresponding barrier - remember the indexed times here are equivalent to the cusum filtered times, and the end barriers are approx a day later\n",
    "        df0 = (df0/close[loc] - 1) * events.at[loc,'side'] # checks all the returns at once - comparing to the starting price at the beginning of this triple barrier scan\n",
    "        out.loc[loc,'sl'] = df0[df0<sl[loc]].index.min()\n",
    "        out.loc[loc,'pt'] = df0[df0>pt[loc]].index.min() # not sl and pt are constant across a barrier search ('box') - taken as the sl/pt at the time of box creation\n",
    "    return out\n",
    "\n",
    "def get_events(close, t_events, ptSl, trgt, minRet, numThreads, t1 = False, side = None):\n",
    "    # If side is None, it means we are trying to learn the side (position) we should take, if we already know the side and are getting the events for the labelling of a secondary model, we will have an argument for side\n",
    "    \n",
    "    trgt = trgt.loc[t_events] # ensures we only care about the trgt values for which our cusum filter has identified\n",
    "    trgt = trgt[trgt>minRet] # not particularly important, just some lower limit of return required\n",
    "\n",
    "    if t1 is False:\n",
    "        t1 = pd.Series(pd.NaT,index=t_events)\n",
    "    if side is None:\n",
    "        side = pd.Series(1,index=trgt.index)\n",
    "    events = pd.concat({'t1': t1, 'trgt': trgt, 'side': side}, axis=1).dropna(subset=['trgt'])\n",
    "    df0 = apply_ptsl_on_t1(close, events, ptSl, events.index)\n",
    "  #  df0 = mpPandsObj(func=applyPtSlOnT1, pdObj=('molecule', events.index), numThreads=numThreads, close=close, events=events, ptSl=ptSl) # this function will be implemented in final chapter\n",
    "    events['t1'] = df0.dropna(how='all').min(axis=1) # finds the first barrier touch out of either it being a vertical, sl, or pt\n",
    "    if side is None:\n",
    "        events = events.drop('side', axis=1)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447bac7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log close</th>\n",
       "      <th>log std</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:17.013</th>\n",
       "      <td>2.319520</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>10.170790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:26.037</th>\n",
       "      <td>2.319422</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>10.169790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:27.004</th>\n",
       "      <td>2.319451</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>10.170085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:29.486</th>\n",
       "      <td>2.319476</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>10.170340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:36.650</th>\n",
       "      <td>2.319458</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>10.170160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:53:24.510</th>\n",
       "      <td>2.351642</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>10.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:54:26.787</th>\n",
       "      <td>2.351404</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:34.797</th>\n",
       "      <td>2.351594</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:56:22.928</th>\n",
       "      <td>2.351575</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.502095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:58:27.413</th>\n",
       "      <td>2.351637</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>10.502750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2362563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         log close   log std      close\n",
       "UTC                                                    \n",
       "2019-01-01 23:05:17.013   2.319520  0.000154  10.170790\n",
       "2019-01-01 23:05:26.037   2.319422  0.000155  10.169790\n",
       "2019-01-01 23:05:27.004   2.319451  0.000154  10.170085\n",
       "2019-01-01 23:05:29.486   2.319476  0.000152  10.170340\n",
       "2019-01-01 23:05:36.650   2.319458  0.000148  10.170160\n",
       "...                            ...       ...        ...\n",
       "2019-12-31 21:53:24.510   2.351642  0.000275  10.502800\n",
       "2019-12-31 21:54:26.787   2.351404  0.000274  10.500300\n",
       "2019-12-31 21:55:34.797   2.351594  0.000274  10.502300\n",
       "2019-12-31 21:56:22.928   2.351575  0.000274  10.502095\n",
       "2019-12-31 21:58:27.413   2.351637  0.000274  10.502750\n",
       "\n",
       "[2362563 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cccd4a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>trgt</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:26.037</th>\n",
       "      <td>2019-01-02 23:05:29.307</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:29.486</th>\n",
       "      <td>2019-01-02 23:05:30.501</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:46.040</th>\n",
       "      <td>2019-01-02 23:06:44.209</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:05:49.812</th>\n",
       "      <td>2019-01-02 23:06:44.209</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:07:17.985</th>\n",
       "      <td>2019-01-02 23:07:19.294</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:52:30.784</th>\n",
       "      <td>NaT</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:53:24.510</th>\n",
       "      <td>NaT</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:54:26.787</th>\n",
       "      <td>NaT</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:34.797</th>\n",
       "      <td>NaT</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:58:27.413</th>\n",
       "      <td>NaT</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264296 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             t1      trgt  side\n",
       "2019-01-01 23:05:26.037 2019-01-02 23:05:29.307  0.000155   NaN\n",
       "2019-01-01 23:05:29.486 2019-01-02 23:05:30.501  0.000152   NaN\n",
       "2019-01-01 23:05:46.040 2019-01-02 23:06:44.209  0.000146   NaN\n",
       "2019-01-01 23:05:49.812 2019-01-02 23:06:44.209  0.000146   NaN\n",
       "2019-01-01 23:07:17.985 2019-01-02 23:07:19.294  0.000148   NaN\n",
       "...                                         ...       ...   ...\n",
       "2019-12-31 21:52:30.784                     NaT  0.000274   1.0\n",
       "2019-12-31 21:53:24.510                     NaT  0.000275   1.0\n",
       "2019-12-31 21:54:26.787                     NaT  0.000274   1.0\n",
       "2019-12-31 21:55:34.797                     NaT  0.000274   1.0\n",
       "2019-12-31 21:58:27.413                     NaT  0.000274   1.0\n",
       "\n",
       "[264296 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get triple barriers\n",
    "trgt = final_bars['log std']\n",
    "events = get_events(final_bars['log close'], t_events, [2,2], trgt, 0.0001, 1, t1, side)\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e479fa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>trgt</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:39.163</th>\n",
       "      <td>2019-01-02 06:07:09.062</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:40.277</th>\n",
       "      <td>2019-01-02 06:07:07.950</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:42.445</th>\n",
       "      <td>2019-01-02 06:05:02.559</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:43.221</th>\n",
       "      <td>2019-01-02 06:04:38.413</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:45.408</th>\n",
       "      <td>2019-01-02 06:04:38.413</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:28:32.819</th>\n",
       "      <td>2019-12-31 21:31:09.938</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:28:34.811</th>\n",
       "      <td>2019-12-31 21:31:09.938</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:28:38.066</th>\n",
       "      <td>2019-12-31 21:31:09.938</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:30:00.566</th>\n",
       "      <td>2019-12-31 21:30:33.796</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:30:04.142</th>\n",
       "      <td>2019-12-31 21:30:37.256</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264160 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             t1      trgt  side\n",
       "2019-01-01 23:11:39.163 2019-01-02 06:07:09.062  0.000269   1.0\n",
       "2019-01-01 23:11:40.277 2019-01-02 06:07:07.950  0.000270   1.0\n",
       "2019-01-01 23:11:42.445 2019-01-02 06:05:02.559  0.000271   1.0\n",
       "2019-01-01 23:11:43.221 2019-01-02 06:04:38.413  0.000274  -1.0\n",
       "2019-01-01 23:11:45.408 2019-01-02 06:04:38.413  0.000281  -1.0\n",
       "...                                         ...       ...   ...\n",
       "2019-12-31 21:28:32.819 2019-12-31 21:31:09.938  0.000147   1.0\n",
       "2019-12-31 21:28:34.811 2019-12-31 21:31:09.938  0.000135   1.0\n",
       "2019-12-31 21:28:38.066 2019-12-31 21:31:09.938  0.000121   1.0\n",
       "2019-12-31 21:30:00.566 2019-12-31 21:30:33.796  0.000121   1.0\n",
       "2019-12-31 21:30:04.142 2019-12-31 21:30:37.256  0.000143  -1.0\n",
       "\n",
       "[264160 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = events.dropna()\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a74ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "side\n",
       " 1.0    135882\n",
       "-1.0    128278\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events['side'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58086e3",
   "metadata": {},
   "source": [
    "#### Step 5: Meta labelling - remember we may want some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c425124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(events, close):\n",
    "    '''\n",
    "    events.index: each triple barrier events start time\n",
    "    events['t1']: events end time (the barrier hit)\n",
    "    events['trgt']: horizontal barriers\n",
    "    events['side'] (optional): pre-found side learnt from (separate) primary model \n",
    "    '''\n",
    "    events = events.dropna(subset=['t1'])\n",
    "    px = events.index.union(events['t1'].values).drop_duplicates() # taking all the index times and event times into one set and dropping any duplicates (as we will only need prices at these times)\n",
    "    px = close.reindex(px, method='bfill') # just take the close prices at the relevant times - as defined above - then just deal with the odd nan by backfilling\n",
    "    \n",
    "    out = pd.DataFrame(index=events.index) # dataframe will have all the barrier hits from the events dataframe (as returned by the getEvents function)\n",
    "    out['ret'] = px.loc[events['t1'].values].values - px.loc[events.index]  # log return takes return from the initial indexed time to the barrier hit\n",
    "    out['hit'] = np.where((out['ret'] > events['trgt']) | (out['ret'] < -1*events['trgt']), 1, 0)\n",
    "    \n",
    "    if 'side' in events:\n",
    "        out['ret'] *= events['side'] # we are not double multiplying here - remember these returns have been calculated fresh from the close in this function\n",
    "    out['bin'] = np.sign(out['ret']) # remember this is the return specifically for barrier hits\n",
    "    \n",
    "    if 'side' in events:\n",
    "        out.loc[out['ret'] <= 0, 'bin'] = 0\n",
    "    else:\n",
    "        out['bin'] = np.where(out['hit'] == 1, out['bin'], 0)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d23c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = get_bins(events, final_bars['log close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f061b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>hit</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:39.163</th>\n",
       "      <td>0.001262</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:40.277</th>\n",
       "      <td>0.001253</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:42.445</th>\n",
       "      <td>0.001269</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:43.221</th>\n",
       "      <td>-0.001279</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 23:11:45.408</th>\n",
       "      <td>-0.001318</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:28:32.819</th>\n",
       "      <td>0.000695</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:28:34.811</th>\n",
       "      <td>0.000734</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:28:38.066</th>\n",
       "      <td>0.000679</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:30:00.566</th>\n",
       "      <td>-0.000573</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:30:04.142</th>\n",
       "      <td>0.000682</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264160 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ret  hit  bin\n",
       "2019-01-01 23:11:39.163  0.001262    1  1.0\n",
       "2019-01-01 23:11:40.277  0.001253    1  1.0\n",
       "2019-01-01 23:11:42.445  0.001269    1  1.0\n",
       "2019-01-01 23:11:43.221 -0.001279    1  0.0\n",
       "2019-01-01 23:11:45.408 -0.001318    1  0.0\n",
       "...                           ...  ...  ...\n",
       "2019-12-31 21:28:32.819  0.000695    1  1.0\n",
       "2019-12-31 21:28:34.811  0.000734    1  1.0\n",
       "2019-12-31 21:28:38.066  0.000679    1  1.0\n",
       "2019-12-31 21:30:00.566 -0.000573    1  0.0\n",
       "2019-12-31 21:30:04.142  0.000682    1  1.0\n",
       "\n",
       "[264160 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c07802f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins bin\n",
      "0.0    135168\n",
      "1.0    128992\n",
      "Name: count, dtype: int64\n",
      "hits hit\n",
      "1    262717\n",
      "0      1443\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('bins', bins['bin'].value_counts())\n",
    "print('hits', bins['hit'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd8e8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.to_csv('target_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd53823",
   "metadata": {},
   "source": [
    "#### Step 6: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40b644",
   "metadata": {},
   "source": [
    "When examining features originating from the time series, we want a time series that has better statistical properties, i.e. is stationary. However, fully differencing the data can result in memory and therefore information loss, therefore we wish to fractionally difference the data just enough so that it is classed as stationary within the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import currencies we will examine and will also be fractionally differenced\n",
    "eurusd_tick_data = pd.read_csv(\"EURUSD_tick_UTC+0_00_2019-Parse.csv.zst\")\n",
    "usdsek_tick_data = pd.read_csv(\"USDSEK_tick_UTC+0_00_2019-Parse.csv.zst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eurusd_tick_data['AskVolume'] = eurusd_tick_data['AskVolume'].groupby(eurusd_tick_data.index // 10).cumsum()\n",
    "usdsek_tick_data['BidVolume'] = usdsek_tick_data['BidVolume'].groupby(usdsek_tick_data.index // 10).cumsum()\n",
    "usdsek_tick_data['AskVolume'] = usdsek_tick_data['AskVolume'].groupby(usdsek_tick_data.index // 10).cumsum()\n",
    "eurusd_tick_data['BidVolume'] = eurusd_tick_data['BidVolume'].groupby(eurusd_tick_data.index // 10).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5048a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data\n",
    "features = features.drop('UTC',axis=1)\n",
    "features.index.tz_localize(None)\n",
    "features.columns = features.columns + 'EURSEK'\n",
    "\n",
    "eurusd_tick_data.index = pd.to_datetime(eurusd_tick_data['UTC'])\n",
    "eurusd_tick_data.drop('UTC',axis=1)\n",
    "eurusd_tick_data.index.tz_localize(None)\n",
    "eurusd_tick_data.columns = eurusd_tick_data.columns + 'EURUSD'\n",
    "\n",
    "usdsek_tick_data.index = pd.to_datetime(usdsek_tick_data['UTC'])\n",
    "usdsek_tick_data.drop('UTC',axis=1)\n",
    "usdsek_tick_data.index.tz_localize(None)\n",
    "usdsek_tick_data.columns = usdsek_tick_data.columns + 'USDSEK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge_asof(features, eurusd_tick_data, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ecf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge_asof(features, usdsek_tick_data, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79120b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop(['UTCEURUSD','UTCUSDSEK'], axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['MidPriceEURUSD'] = (features['BidPriceEURUSD'] + features['AskPriceEURUSD'])/2\n",
    "features['MidPriceUSDSEK'] = (features['BidPriceUSDSEK'] + features['AskPriceUSDSEK'])/2\n",
    "features[['logCloseEURSEK','logCloseUSDSEK','logCloseEURUSD']] = np.log(features[['MidPriceEURSEK','MidPriceUSDSEK','MidPriceEURUSD']])\n",
    "features[['logStdEURSEK','logStdUSDSEK','logStdEURUSD']] = features[['logCloseEURSEK','logCloseUSDSEK','logCloseEURUSD']].rolling(50).std()\n",
    "features = features.dropna()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeights(d, size):\n",
    "    w = [1.]\n",
    "    for k in range(1,size):\n",
    "        w_=-w[-1]*(d-k+1)/k\n",
    "        w.append(w_)\n",
    "    w=np.array(w[::-1]).reshape(-1,1)\n",
    "    return w\n",
    "\n",
    "def fracDiff(series, d, thres=0.01):\n",
    "    # 1 - Compute weights for longest series, ie weights up until T\n",
    "    w = getWeights(d, series.shape[0])  # note that the 1, indicating the 'first' binomial weight; the one also attached to the most recent data point, is the final element in the array.\n",
    "    # 2 - Weight loss threshold calculations\n",
    "    w_=np.cumsum(abs(w))\n",
    "    lamda=w_/w_[-1]\n",
    "    skip=lamda[lamda>thres].shape[0] # the higher the threshold, the fewer points are greater than the tthreshold, therefore we 'skip' fewer points, so ou skip index (used below) starts from a lower value, as we skip fewer initial points\n",
    "    #3 - apply weights to find each fractionally differentiated value, excluding the skipped values\n",
    "    df={}\n",
    "    for name in series.columns:\n",
    "        seriesF = series[[name]].fillna(method='ffill').dropna() # going through supplied dataframe column by column\n",
    "        X_tilda=pd.Series(index=seriesF.iloc[skip:seriesF.shape[0]].index)\n",
    "        for iloc in range(skip,seriesF.shape[0]): # ranging from after skipped values to most recent values\n",
    "            loc=seriesF.index[iloc] # time of current int index\n",
    "            if not np.isfinite(series.loc[loc,name]):\n",
    "                continue\n",
    "            # Now we look at the current time for which we want to calculate X_tilda\n",
    "            X_tilda[loc] = np.dot(w[-(iloc+1):,:].T, seriesF.loc[:loc])[0,0] # dotting the latter weights in the transposed array (so the last weight here will still be 1) with all the past times up until the current time\n",
    "        df[name] = X_tilda.copy(deep=True)\n",
    "    df=pd.concat(df,axis=1)\n",
    "    return df\n",
    "\n",
    "def getWeights_FFD(d,thres):\n",
    "    w = [1.]\n",
    "    k=1\n",
    "    while w[-1] >= thres:\n",
    "        w_=-w[-1]*(d-k+1)/k\n",
    "        w.append(w_)\n",
    "        k+=1\n",
    "    w=np.array(w[::-1]).reshape(-1,1)\n",
    "    return w\n",
    "\n",
    "def fracDiff_FFD(series, d, thres=1e-5):\n",
    "    # 1 - Compute weights for longest series, ie weights up until T\n",
    "    print('getting weights with d=',d)\n",
    "    w=getWeights_FFD(d,thres)\n",
    "    print('weights obtained')\n",
    "    width=len(w)-1 # this is how many memory terms we will have for X tilda at all t\n",
    "    # 2 - apply weights to find each fractionally differentiated value, excluding the skipped (equal to the width) values\n",
    "    df={}\n",
    "    for name in series.columns: # for if there are multiple time series in our df we wish to difference\n",
    "        seriesF = series[[name]].fillna(method='ffill').dropna()\n",
    "        X_tilda=pd.Series(index=seriesF.iloc[width:seriesF.shape[0]].index)\n",
    "        for iloc1 in range(width,seriesF.shape[0]):\n",
    "            loc0, loc1 = seriesF.index[iloc1-width], seriesF.index[iloc1] # times at beginning of window and end of window\n",
    "            if not np.isfinite(series.loc[loc1,name]):\n",
    "                continue\n",
    "            X_tilda[loc1] = np.dot(w.T, seriesF.loc[loc0:loc1])[0,0]\n",
    "        df[name]=X_tilda.copy(deep=True)\n",
    "    df=pd.concat(df,axis=1)\n",
    "    return df\n",
    "\n",
    "def plotMinFFD(series,column_plot,thres=1e-2):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    out=pd.DataFrame(columns=['adfStat','pVal','lags','nObs','95% conf','corr'])\n",
    "    for d in np.linspace(0,0.7,8):\n",
    "        df1=np.log(series[[column_plot]]).resample('1D').last() # resamples series to daily instead of whatever it was before and uses log prices\n",
    "        df1=np.log(series[[column_plot]])\n",
    "        df1=series[[column_plot]]\n",
    "        df2=fracDiff_FFD(df1,d,thres) # our new fractionally differentiated series using fixed window method\n",
    "        corr=np.corrcoef(df1.loc[df2.index,column_plot],df2[column_plot])[0,1] # correlation between differenced series and original series - we want this to be high as this indicates the series are similar with regards to information, therefore we have not lost a lot of memory for high correlation coefficients\n",
    "        df2=adfuller(df2[column_plot],maxlag=1,regression='c',autolag=None)\n",
    "        out.loc[d]=list(df2[:4]) + [df2[4]['5%']] + [corr] # just takes returns from adfuller function and puts into nice list\n",
    "    plt.figure()\n",
    "    out[['adfStat','corr']].plot(secondary_y='adfStat',xlabel='Fraction Differentiated Amount',ylabel='adf stat')\n",
    "    plt.grid(axis='both')\n",
    "    plt.axhline(out['95% conf'].mean(),linewidth=1,color='r',linestyle='dotted')\n",
    "    plt.title(column_plot)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d08668",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMinFFD(features,'logCloseEURSEK',1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89577e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['logCloseEURSEKFracDiff'] = fracDiff_FFD(features[['logCloseEURSEK']],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd576b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p-value: ', adfuller(features['logCloseEURSEKFracDiff'],maxlag=1,regression='c',autolag=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9942be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['logCloseUSDSEKFracDiff'] = fracDiff_FFD(features[['logCloseUSDSEK']],0.4)\n",
    "features = features.dropna()\n",
    "print('p-value: ', adfuller(features['logCloseUSDSEKFracDiff'],maxlag=1,regression='c',autolag=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['logCloseEURUSDFracDiff'] = fracDiff_FFD(features[['logCloseEURUSD']],0.2)\n",
    "features = features.dropna()\n",
    "print('p-value: ', adfuller(features['logCloseEURUSDFracDiff'],maxlag=1,regression='c',autolag=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc516ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = series_fracdiff\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca1d56",
   "metadata": {},
   "source": [
    "#### Step 6.1 - TS features (self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23788dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility\n",
    "features['volatility_50_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=50).std()\n",
    "features['volatility_25_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=25).std()\n",
    "features['volatility_10_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=10).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "features['autocorr_1_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "print('done lag 1')\n",
    "features['autocorr_2_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "print('done lag 2')\n",
    "features['autocorr_3_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "print('done lag 3')\n",
    "features['autocorr_4_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=4), raw=False)\n",
    "print('done lag 4')\n",
    "features['autocorr_5_EURSEK_fracDiff'] = features['logCloseEURSEKFracDiff'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=5), raw=False)\n",
    "print('done lag 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99701e93",
   "metadata": {},
   "source": [
    "#### Step 6.2: Microstructural indicators (self and via USD currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spread, volume imbalance etc\n",
    "\n",
    "# Spread\n",
    "features['spread_EURSEK'] = features['AskPriceEURSEK'] - features['BidPriceEURSEK']\n",
    "features['spread_USDSEK'] = features['AskPriceUSDSEK'] - features['BidPriceUSDSEK']\n",
    "features['spread_EURUSD'] = features['AskPriceEURUSD'] - features['BidPriceEURUSD']\n",
    "\n",
    "# Volume imbalance\n",
    "features['volume_imbalance_EURSEK'] = features['BidVolumeEURSEK'] - features['AskVolumeEURSEK']\n",
    "features['volume_imbalance_USDSEK'] = features['BidVolumeUSDSEK'] - features['AskVolumeUSDSEK']\n",
    "features['volume_imbalance_EURUSD'] = features['BidVolumeEURUSD'] - features['AskVolumeEURUSD']\n",
    "features['volume_sum_EURSEK'] = features['BidVolumeEURSEK'] + features['AskVolumeEURSEK']\n",
    "features['volume_sum_USDSEK'] = features['BidVolumeUSDSEK'] + features['AskVolumeUSDSEK']\n",
    "features['volume_sum_EURUSD'] = features['BidVolumeEURUSD'] + features['AskVolumeEURUSD']\n",
    "features['volume_imbalance_EURSEK_normalized'] = features['volume_imbalance_EURSEK']/features['volume_sum_EURSEK']\n",
    "features['volume_imbalance_USDSEK_normalized'] = features['volume_imbalance_USDSEK']/features['volume_sum_USDSEK']\n",
    "features['volume_imbalance_EURUSD_normalized'] = features['volume_imbalance_EURUSD']/features['volume_sum_EURUSD']\n",
    "features['EURUSD_to_USDSEK_flow'] = features['volume_imbalance_EURUSD_normalized'] + features['volume_imbalance_USDSEK_normalized']\n",
    "\n",
    "# Picing mismatch\n",
    "features['crossPrice'] = features['MidPriceEURUSD'] * features['MidPriceUSDSEK']\n",
    "features['priceMisMatch'] = features['crossPrice'] - features['MidPriceEURSEK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed815d5",
   "metadata": {},
   "source": [
    "#### Step 6.3: Entropy indicators (self and via USD currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from AFML\n",
    "def quartile_encode(df, column):\n",
    "    quartile_labels = ['1', '2', '3', '4']\n",
    "\n",
    "    # Use qcut to create a new column with quartile labels\n",
    "    df[column + '_quartile'] = pd.qcut(df[column], q=4, labels=quartile_labels)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def match_length(msg, i, n):\n",
    "    subS=''\n",
    "    for l in range(n):\n",
    "        msg1 = msg[i:i+l+1]\n",
    "        for j in range(i-n, i):\n",
    "            msg0 = msg[j:j+l+1]\n",
    "            if msg1==msg0:\n",
    "                subS = msg1\n",
    "                break\n",
    "    return len(subS)+1, subS\n",
    "\n",
    "def konto(msg, window=None):\n",
    "    out = {'num':0, 'sum':0, 'subS':[]}\n",
    "    if not isinstance(msg,str):\n",
    "        msg=''.join(map(str,msg))\n",
    "    if window is None:\n",
    "        points = range(1,len(msg)//2+1)\n",
    "    else:\n",
    "        window = min(window, len(msg)/2)\n",
    "        points = range(window, len(msg)-window+1)\n",
    "    for i in points:\n",
    "        if window is None:\n",
    "            l, msg_ = match_length(msg, i, i)\n",
    "            out['sum']+=np.log2(i+1)/l\n",
    "        else:\n",
    "            l, msg_ = match_length(msg, i, window)\n",
    "            out['sum']+=np.log2(window+1)/l\n",
    "        out['subS'].append(msg_)\n",
    "        out['num']+=1\n",
    "        if i%100000 == 0:\n",
    "            print(i)\n",
    "    out['h'] = out['sum']/out['num']\n",
    "    out['r'] = 1-out['h']/np.log2(len(msg))\n",
    "    return out\n",
    "\n",
    "msg = '1234432342123341233'\n",
    "konto(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394adda",
   "metadata": {},
   "source": [
    "Will just use scipy entropy for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8191236",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['entropyEURSEK'] = features['logCloseEURSEKFracDiff'].rolling(window=100).apply(lambda x: sp.stats.entropy(np.histogram(x, bins='auto')[0]))\n",
    "features['entropyUSDSEK'] = features['logCloseUSDSEKFracDiff'].rolling(window=100).apply(lambda x: sp.stats.entropy(np.histogram(x, bins='auto')[0]))\n",
    "features['entropyEURUSD'] = features['logCloseEURUSDFracDiff'].rolling(window=100).apply(lambda x: sp.stats.entropy(np.histogram(x, bins='auto')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.index = features.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1041221",
   "metadata": {},
   "outputs": [],
   "source": [
    "sides.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(sides, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.shift(1) # remove look ahead bias\n",
    "features = features.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8390b",
   "metadata": {},
   "source": [
    "#### Step7: Model with sample weighting\n",
    "Try both random fotest and bagged SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = bins.join(features, how='inner')\n",
    "model_data = model_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da561c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.to_csv(\"eursek_signal_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_data['bin']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29249ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[['spread_EURSEK', 'spread_USDSEK', 'spread_EURUSD', \n",
    "                'volume_imbalance_EURSEK_normalized', 'EURUSD_to_USDSEK_flow', \n",
    "                'priceMisMatch', 'entropyEURSEK', 'entropyUSDSEK',\n",
    "                'volatility_50_EURSEK_fracDiff', 'volatility_25_EURSEK_fracDiff', 'volatility_10_EURSEK_fracDiff',\n",
    "                'autocorr_1_EURSEK_fracDiff', 'autocorr_2_EURSEK_fracDiff',\n",
    "                'autocorr_3_EURSEK_fracDiff', 'autocorr_4_EURSEK_fracDiff', 'autocorr_5_EURSEK_fracDiff','side_confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa27914",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81822c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36ebf2",
   "metadata": {},
   "source": [
    "Sample weight the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpNumCoEvents(closeIdx,t1,molecule):\n",
    "    count = pd.Series(0, index=closeIdx) # just ensures we use the time indices that occur AFTER the closest time indices to t1 start and finish (t_i,0 to the very last time entry in t1)\n",
    "    for tIn, tOut in t1.items():\n",
    "        count.loc[tIn:tOut] += 1 # any times from the close data indices between the triple barrier times get a +1 (so we form a count for each close index time as to how many times they appear in a triple barrier interval)\n",
    "    return count\n",
    "\n",
    "def mpSampleTW(t1,numCoEvents,molecule):\n",
    "    wght = pd.Series(0, index=t1.index)\n",
    "    wght = wght.loc[molecule[0]:molecule[1]]\n",
    "    for tIn, tOut in t1.loc[wght.index].items():\n",
    "        # wght.index gives the full range of times the function will provide\n",
    "        # here we go through each triple barrier interval and find the mean 1/count for each interval (where as we are checking each specific triple barrier interval, the indicator is of course always 1 for that interval)\n",
    "        wght.loc[tIn] = (1./numCoEvents.loc[tIn:tOut]).mean()\n",
    "    return wght\n",
    "\n",
    "def mpSampleW(t1, numCoEvents, close, molecule):\n",
    "    '''\n",
    "    t1: The triple barrier times\n",
    "    numCoEvents: The concurrency table indexed by raw data times (t, not i)\n",
    "    close: The actual raw data for prices (using whatever bars the data is supplied in)\n",
    "    molecule: The list of triple barrier (i) indices that we will use in this particular parallel processing iteration\n",
    "    '''\n",
    "    ret = np.log(close).diff() # log returns to ensure additivity\n",
    "    wght = pd.Series(index=molecule)\n",
    "    for tIn, tOut in t1.loc[wght.index].items():\n",
    "        wght.loc[tIn] = (ret.loc[tIn:tOut]/numCoEvents.loc[tIn:tOut]).sum() # calculates the weight given in notes for each i\n",
    "    return wght.abs() #returns weights pre normalization\n",
    "\n",
    "\n",
    "def getTimeDecay(tW, clfLastW=1):\n",
    "    '''\n",
    "    tW: The table of uniqueness foreach barrier\n",
    "    clfLastW: The user chosen c constant\n",
    "    '''\n",
    "    clfW = tW.sort_index().cumsum() # sorts the index for the labels and takes the cumulative sum of the uniqueness (see notes)\n",
    "    # Find a and b as calculated in notes according to BCs\n",
    "    if clfLastW >= 0:\n",
    "        b = (1-clfLastW)/clfW.iloc[-1]\n",
    "    else:\n",
    "        b = 1 / ((clfLastW+1)*clfW.iloc[-1])\n",
    "    a = 1 - b*clfW.iloc[-1]\n",
    "    clfW = a + b*clfW # actually calculates our d function for all the values of cumulative uniqueness\n",
    "    clfW[clfW < 0] = 0 # applies our max function\n",
    "    print(a,b)\n",
    "    return clfW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d52b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_train = events.loc[X_train.index]\n",
    "concurrency = mpNumCoEvents(final_bars.index,events_train['t1'],[events_train.index[0],events_train['t1'].iloc[-1]])\n",
    "avg_uniqueness = mpSampleTW(events_train['t1'],concurrency,[events_train.index[0],events_train['t1'].iloc[-1]])\n",
    "weights_unnormalized=mpSampleW(events_train['t1'], concurrency, final_bars['close'],events_train['t1'].index)\n",
    "td=getTimeDecay(avg_uniqueness,0.7)\n",
    "weights_unnormalized_td=weights_unnormalized*td\n",
    "weights_normalized=weights_unnormalized*weights_unnormalized.shape[0]/weights_unnormalized.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "td=getTimeDecay(avg_uniqueness,0.7)\n",
    "weights_unnormalized_td=weights_unnormalized*td\n",
    "weights_normalized=weights_unnormalized*weights_unnormalized.shape[0]/weights_unnormalized.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6199b3",
   "metadata": {},
   "source": [
    "Now fit model with sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91425d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(X_train, X_test, y_train, y_test, clf):\n",
    "    y_positive_always = np.ones(len(X_test))\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('Comparison')\n",
    "\n",
    "    print('')\n",
    "    print('Always trade')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test,y_positive_always))\n",
    "\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(y_test, y_positive_always))\n",
    "    print('Recall:')\n",
    "    print(recall_score(y_test, y_positive_always))\n",
    "    print('Precision:')\n",
    "    print(precision_score(y_test, y_positive_always))\n",
    "    \n",
    "    print('Report')\n",
    "    print(classification_report(y_test,y_positive_always))\n",
    "\n",
    "    print('')\n",
    "    print('Predicted Train')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(y_train,y_train_pred))\n",
    "    print('Recall:')\n",
    "    print(recall_score(y_train,y_train_pred))\n",
    "    print('Precision:')\n",
    "    print(precision_score(y_train,y_train_pred))\n",
    "    \n",
    "    print('Report')\n",
    "    print(classification_report(y_train,y_train_pred))\n",
    "\n",
    "    print('')\n",
    "    print('Predicted Test')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    print('Recall:')\n",
    "    print(recall_score(y_test, y_pred))\n",
    "    print('Precision:')\n",
    "    print(precision_score(y_test, y_pred))\n",
    "    \n",
    "    print('Report')\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27075a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit without any tinkering\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(X_train, y_train, weights_normalized)\n",
    "\n",
    "get_stats(X_train, X_test, y_train, y_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11744381",
   "metadata": {},
   "source": [
    "Lets get a rough idea of the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "importances = pd.Series(importances, index=X_train.columns)\n",
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff1e68",
   "metadata": {},
   "source": [
    "We appear to be overfitting, lets do a PCA to reduce dimensionality..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182649b8",
   "metadata": {},
   "source": [
    "#### Step 8: Feature selection, including possible PCA - as we clearly have overfitting occurring, therefore want featurers independent of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970322d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_84072\\4096517416.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Perform PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Plot the explained variance ratio to see the importance of each component\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Plot the explained variance ratio to see the importance of each component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Component Importance')\n",
    "plt.show()\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Display cumulative explained variance ratio in a table\n",
    "print(\"\\nCumulative Explained Variance Ratio:\")\n",
    "for i, ratio in enumerate(cumulative_variance_ratio):\n",
    "    print(f\"Component {i + 1}: {ratio:.4f}\")\n",
    "\n",
    "# Choose a number of components based on the cumulative explained variance ratio\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fit PCA with the chosen number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same PCA\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27067179",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest on the PCA-transformed training data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_pca, y_train, weights_normalized)\n",
    "\n",
    "get_stats(X_train_pca, X_test_pca, y_train, y_test,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8625b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a XGBoost on the PCA-transformed training data\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(random_state=42)\n",
    "xgb_cl.fit(X_train_pca, y_train, weights_normalized)\n",
    "\n",
    "get_stats(X_train_pca, X_test_pca, y_train, y_test, xgb_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c67dd9",
   "metadata": {},
   "source": [
    "#### Step 9: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd60899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.05],\n",
    "    \"gamma\": [0, 0.25, 1],\n",
    "    \"reg_lambda\": [0, 1, 10],\n",
    "    \"scale_pos_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.5],\n",
    "}\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "grid_cv_xgb = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_xgb.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3980c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf0c44",
   "metadata": {},
   "source": [
    "#### Step 10: Fit final model with selected hyperparameters and possible PCA components, test on test data (perhaps bring in some proper backtesting principles here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46398787",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cl_final = xgb.XGBClassifier(random_state=42)\n",
    "xgb_cl_final.fit(X_train_pca, y_train, weights_normalized)\n",
    "\n",
    "get_stats(X_train_pca, X_test_pca, y_train, y_test, xgb_cl_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_prob = xgb_cl_final.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = {:.2f})'.format(auc))\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d0745",
   "metadata": {},
   "source": [
    "Not sure why tghis changed slighly when re-running.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_cl_final.predict(X_test_pca)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afe52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_events = bins.loc[y_test.index]\n",
    "test_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_events['bin_pred'] = y_pred\n",
    "test_events['trade_always'] = np.ones(len(test_events))\n",
    "test_events['ret_trade'] = test_events['ret']*test_events['bin_pred']\n",
    "test_events['ret_trade_always'] = test_events['ret']*test_events['trade_always']\n",
    "test_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_model = test_events[test_events['ret_trade'] != 0.]\n",
    "print('Model Sharpe Ratio:', trades_model['ret_trade'].mean()/trades_model['ret_trade'].std())\n",
    "print('Always Trade Sharpe Ratio:', test_events['ret_trade_always'].mean()/test_events['ret_trade_always'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8f963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745db213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
